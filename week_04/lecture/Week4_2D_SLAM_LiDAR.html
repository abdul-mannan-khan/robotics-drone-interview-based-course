<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Week 4: 2D SLAM with LiDAR - TC70045E</title>
    <script>
        MathJax = {
            tex: { inlineMath: [['\\(', '\\)']], displayMath: [['\\[', '\\]']] },
            svg: { fontCache: 'global' }
        };
    </script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" async></script>
    <style>
        @page { size: A4; margin: 2cm; }
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; color: #333; background: #f5f5f5; }
        .page { width: 21cm; min-height: 29.7cm; padding: 2cm; margin: 20px auto; background: white; box-shadow: 0 0 10px rgba(0,0,0,0.1); page-break-after: always; }
        .header { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; text-align: center; border-radius: 10px; margin-bottom: 30px; }
        .header h1 { font-size: 2.5em; margin-bottom: 10px; }
        .header p { font-size: 1.2em; opacity: 0.9; }
        h2 { color: #667eea; border-bottom: 3px solid #764ba2; padding-bottom: 10px; margin: 30px 0 20px 0; font-size: 1.8em; }
        h3 { color: #764ba2; margin: 25px 0 15px 0; font-size: 1.4em; }
        h4 { color: #667eea; margin: 20px 0 10px 0; font-size: 1.2em; }
        .info-box { background: #e8f4f8; border-left: 5px solid #667eea; padding: 20px; margin: 20px 0; border-radius: 5px; }
        .warning-box { background: #fff4e6; border-left: 5px solid #ff9800; padding: 20px; margin: 20px 0; border-radius: 5px; }
        .activity-box { background: #f0f8ff; border: 3px solid #4CAF50; padding: 25px; margin: 30px 0; border-radius: 10px; }
        .activity-box h3 { color: #4CAF50; margin-top: 0; }
        .image-container { text-align: center; margin: 30px 0; }
        .image-container img { max-width: 100%; height: auto; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }
        .image-caption { font-style: italic; color: #666; margin-top: 10px; font-size: 0.9em; }
        ul, ol { margin: 15px 0 15px 30px; }
        li { margin: 8px 0; }
        .two-column { display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0; }
        .three-column { display: grid; grid-template-columns: 1fr 1fr 1fr; gap: 15px; margin: 20px 0; }
        .checklist { background: #f9f9f9; padding: 20px; border-radius: 8px; margin: 20px 0; }
        .checklist li { list-style: none; padding: 8px 0; }
        .checklist li:before { content: "\2713 "; color: #4CAF50; font-weight: bold; margin-right: 10px; }
        table { width: 100%; border-collapse: collapse; margin: 20px 0; table-layout: fixed; word-wrap: break-word; }
        th, td { border: 1px solid #ddd; padding: 12px; text-align: left; }
        th { background: #667eea; color: white; }
        tr:nth-child(even) { background: #f9f9f9; }
        .diagram { border: 2px solid #667eea; border-radius: 10px; padding: 20px; margin: 20px 0; background: white; }
        .flow-step { background: #667eea; color: white; padding: 15px; margin: 10px 0; border-radius: 8px; text-align: center; font-weight: bold; }
        .arrow { text-align: center; font-size: 2em; color: #764ba2; margin: 5px 0; }
        .tip { background: #fffbea; border-left: 5px solid #ffd700; padding: 15px; margin: 15px 0; border-radius: 5px; }
        .tip:before { content: "\1F4A1 TIP: "; font-weight: bold; color: #ff9800; }
        code { background: #f4f4f4; padding: 2px 6px; border-radius: 3px; font-family: 'Courier New', monospace; word-break: break-all; }
        pre { background: #1e1e1e; color: #d4d4d4; padding: 20px; border-radius: 8px; overflow-x: auto; margin: 15px 0; font-family: 'Courier New', monospace; font-size: 0.9em; line-height: 1.5; max-width: 100%; white-space: pre-wrap; word-wrap: break-word; }
        .example-box { background: #f0fff0; border: 2px dashed #4CAF50; padding: 20px; margin: 20px 0; border-radius: 8px; }
        .math { padding: 15px; margin: 15px 0; border-radius: 5px; overflow-x: auto; max-width: 100%; }
        .sensor-card { background: white; border: 2px solid #667eea; border-radius: 10px; padding: 20px; text-align: center; }
        .sensor-card h4 { color: #667eea; margin-bottom: 10px; }
        img { max-width: 100%; height: auto; }
        svg { max-width: 100%; height: auto; }
        .two-column > *, .three-column > * { min-width: 0; overflow: hidden; }
        td code { word-break: break-all; }
    </style>
</head>
<body>

<!-- ==================== PAGE 1: TITLE PAGE ==================== -->
<div class="page">
    <div class="header" style="padding: 60px 30px; margin-top: 60px;">
        <h1>2D SLAM with LiDAR</h1>
        <p>Week 4 - Robotics &amp; Drone Engineering</p>
        <p style="margin-top: 15px; font-size: 1em;">TC70045E | M.Sc. Programme</p>
    </div>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1561557944-6e7860d1a7eb?w=800&h=400&fit=crop" alt="Robot navigating and mapping an environment">
        <p class="image-caption">Simultaneous Localization and Mapping enables autonomous robots to build maps while navigating unknown environments</p>
    </div>

    <div style="text-align: center; margin-top: 40px;">
        <h2 style="border: none; text-align: center;">Course Details</h2>
        <table style="width: 80%; margin: 20px auto;">
            <tr><th style="width: 40%;">Field</th><th>Details</th></tr>
            <tr><td><strong>Module Code</strong></td><td>TC70045E</td></tr>
            <tr><td><strong>Programme</strong></td><td>M.Sc. Robotics &amp; Drone Engineering</td></tr>
            <tr><td><strong>Instructor</strong></td><td>Dr. Abdul Manan Khan</td></tr>
            <tr><td><strong>Institution</strong></td><td>University of West London</td></tr>
            <tr><td><strong>Week</strong></td><td>4 - 2D SLAM with LiDAR</td></tr>
            <tr><td><strong>Delivery</strong></td><td>Lecture + Hands-on Lab</td></tr>
        </table>
    </div>

    <div class="info-box" style="margin-top: 30px;">
        <strong>Lecture Overview:</strong> This week we explore the fundamental problem of Simultaneous Localization and Mapping (SLAM) using 2D LiDAR sensors. We will cover the mathematical foundations of occupancy grid mapping, scan matching via ICP, particle filter and graph-based SLAM approaches, loop closure detection, and practical implementation using ROS2 packages.
    </div>
</div>

<!-- ==================== PAGE 2: LEARNING OBJECTIVES & WHAT IS SLAM ==================== -->
<div class="page">
    <h2>Learning Objectives</h2>

    <ul class="checklist">
        <li>Explain the SLAM problem and why it is fundamental to autonomous robotics</li>
        <li>Describe occupancy grid maps and their probabilistic representation using log-odds</li>
        <li>Implement Bresenham's ray casting for LiDAR scan integration</li>
        <li>Understand the Iterative Closest Point (ICP) algorithm for scan matching</li>
        <li>Distinguish between particle filter (FastSLAM) and graph-based SLAM approaches</li>
        <li>Explain loop closure detection and its impact on map consistency</li>
        <li>Configure and run SLAM algorithms in ROS2 using slam_toolbox</li>
        <li>Evaluate SLAM performance using ATE and RPE metrics</li>
    </ul>

    <h2>What is SLAM?</h2>

    <p><strong>Simultaneous Localization and Mapping (SLAM)</strong> is the computational problem of constructing a map of an unknown environment while simultaneously tracking the agent's location within it.</p>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1518314916381-77a37c2a49ae?w=800&h=350&fit=crop" alt="Robot exploring an unknown environment">
        <p class="image-caption">A mobile robot must build a map and localize itself simultaneously in unknown territory</p>
    </div>

    <h3>The Chicken-and-Egg Problem</h3>

    <div class="two-column">
        <div class="info-box">
            <h4>Localization Requires a Map</h4>
            <p>To know <em>where</em> the robot is, we need a map of the environment to match sensor observations against.</p>
        </div>
        <div class="info-box">
            <h4>Mapping Requires a Pose</h4>
            <p>To build an accurate <em>map</em>, we need to know the robot's pose so that sensor readings can be placed correctly in the world frame.</p>
        </div>
    </div>

    <p>SLAM solves both problems jointly. Formally, given the sequence of controls \( u_{1:T} \) and observations \( z_{1:T} \), SLAM estimates the posterior:</p>

    <div class="math">
        \[ p(x_{1:T}, m \mid z_{1:T}, u_{1:T}) \]
    </div>

    <p>where \( x_{1:T} \) is the trajectory of poses and \( m \) is the map.</p>

    <div class="warning-box">
        <strong>Key Insight:</strong> Errors in localization corrupt the map, and errors in the map corrupt localization. SLAM algorithms must carefully manage this coupled uncertainty to avoid catastrophic drift.
    </div>
</div>

<!-- ==================== PAGE 3: SLAM TAXONOMY ==================== -->
<div class="page">
    <h2>SLAM Taxonomy</h2>

    <p>SLAM algorithms can be classified along several axes. Understanding this taxonomy helps you choose the right approach for a given application.</p>

    <!-- SVG Taxonomy Tree -->
    <div class="diagram">
        <svg viewBox="0 0 700 320" xmlns="http://www.w3.org/2000/svg" style="width:100%; height:auto;">
            <!-- Root -->
            <rect x="270" y="10" width="160" height="40" rx="8" fill="#667eea" stroke="#764ba2" stroke-width="2"/>
            <text x="350" y="36" text-anchor="middle" fill="white" font-weight="bold" font-size="14">SLAM Approaches</text>
            <!-- Level 1 branches -->
            <line x1="350" y1="50" x2="175" y2="90" stroke="#764ba2" stroke-width="2"/>
            <line x1="350" y1="50" x2="525" y2="90" stroke="#764ba2" stroke-width="2"/>
            <rect x="90" y="90" width="170" height="36" rx="8" fill="#764ba2"/>
            <text x="175" y="114" text-anchor="middle" fill="white" font-weight="bold" font-size="13">Filter-Based</text>
            <rect x="440" y="90" width="170" height="36" rx="8" fill="#764ba2"/>
            <text x="525" y="114" text-anchor="middle" fill="white" font-weight="bold" font-size="13">Graph-Based</text>
            <!-- Filter children -->
            <line x1="140" y1="126" x2="80" y2="170" stroke="#667eea" stroke-width="1.5"/>
            <line x1="210" y1="126" x2="270" y2="170" stroke="#667eea" stroke-width="1.5"/>
            <rect x="10" y="170" width="140" height="32" rx="6" fill="#e8f4f8" stroke="#667eea" stroke-width="1.5"/>
            <text x="80" y="191" text-anchor="middle" fill="#333" font-size="11">EKF-SLAM</text>
            <rect x="200" y="170" width="140" height="32" rx="6" fill="#e8f4f8" stroke="#667eea" stroke-width="1.5"/>
            <text x="270" y="191" text-anchor="middle" fill="#333" font-size="11">Particle Filter (FastSLAM)</text>
            <!-- Graph children -->
            <line x1="490" y1="126" x2="430" y2="170" stroke="#667eea" stroke-width="1.5"/>
            <line x1="560" y1="126" x2="620" y2="170" stroke="#667eea" stroke-width="1.5"/>
            <rect x="360" y="170" width="140" height="32" rx="6" fill="#e8f4f8" stroke="#667eea" stroke-width="1.5"/>
            <text x="430" y="191" text-anchor="middle" fill="#333" font-size="11">Pose Graph</text>
            <rect x="550" y="170" width="140" height="32" rx="6" fill="#e8f4f8" stroke="#667eea" stroke-width="1.5"/>
            <text x="620" y="191" text-anchor="middle" fill="#333" font-size="11">Factor Graph</text>
            <!-- Map representation row -->
            <rect x="100" y="240" width="200" height="36" rx="8" fill="#4CAF50"/>
            <text x="200" y="264" text-anchor="middle" fill="white" font-weight="bold" font-size="12">Feature-Based Map</text>
            <rect x="400" y="240" width="200" height="36" rx="8" fill="#4CAF50"/>
            <text x="500" y="264" text-anchor="middle" fill="white" font-weight="bold" font-size="12">Direct / Grid Map</text>
            <text x="350" y="300" text-anchor="middle" fill="#764ba2" font-size="12" font-style="italic">Map Representation Axis</text>
        </svg>
    </div>

    <h3>Comparison Table</h3>
    <table>
        <tr>
            <th>Criterion</th>
            <th>Filter-Based</th>
            <th>Graph-Based</th>
        </tr>
        <tr>
            <td><strong>Processing</strong></td>
            <td>Online, incremental</td>
            <td>Can be batch or incremental</td>
        </tr>
        <tr>
            <td><strong>Scalability</strong></td>
            <td>Good for small/medium maps</td>
            <td>Excellent for large environments</td>
        </tr>
        <tr>
            <td><strong>Loop Closure</strong></td>
            <td>Difficult to incorporate retroactively</td>
            <td>Naturally supported via graph edges</td>
        </tr>
        <tr>
            <td><strong>Accuracy</strong></td>
            <td>Subject to linearization errors</td>
            <td>Globally consistent solutions</td>
        </tr>
        <tr>
            <td><strong>Computation</strong></td>
            <td>Constant per step (particle filter)</td>
            <td>Grows with graph size (sparse solvers help)</td>
        </tr>
        <tr>
            <td><strong>Examples</strong></td>
            <td>GMapping, EKF-SLAM</td>
            <td>Cartographer, GTSAM, g2o</td>
        </tr>
    </table>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1589254065878-42c014d66894?w=800&h=300&fit=crop" alt="Sensor data from autonomous vehicle">
        <p class="image-caption">Modern SLAM systems must handle diverse sensor modalities and large-scale environments</p>
    </div>
</div>

<!-- ==================== PAGE 4: OCCUPANCY GRID MAPS ==================== -->
<div class="page">
    <h2>Occupancy Grid Maps</h2>

    <p>An <strong>occupancy grid map</strong> discretises the environment into a grid of cells. Each cell stores the probability that it is occupied. This representation was introduced by Elfes (1989) and remains the dominant 2D map format in robotics.</p>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1558618666-fcd25c85f82e?w=800&h=350&fit=crop" alt="Grid-based environment representation">
        <p class="image-caption">Occupancy grids discretise continuous space into cells with occupancy probabilities</p>
    </div>

    <h3>Probabilistic Representation</h3>

    <p>Each cell \( c_i \) stores a value \( p(c_i) \in [0, 1] \):</p>

    <ul>
        <li>\( p(c_i) = 0 \) means the cell is definitely <strong>free</strong></li>
        <li>\( p(c_i) = 1 \) means the cell is definitely <strong>occupied</strong></li>
        <li>\( p(c_i) = 0.5 \) means <strong>unknown</strong> (no information)</li>
    </ul>

    <h3>Bayesian Update</h3>

    <p>Given a new sensor measurement \( z_t \) and robot pose \( x_t \), the occupancy of cell \( c_i \) is updated using Bayes' rule:</p>

    <div class="math">
        \[ p(c_i \mid z_{1:t}, x_{1:t}) = \frac{p(z_t \mid c_i, x_t) \; p(c_i \mid z_{1:t-1}, x_{1:t-1})}{p(z_t \mid z_{1:t-1}, x_{1:t})} \]
    </div>

    <h3>Inverse Sensor Model</h3>

    <p>The <strong>inverse sensor model</strong> \( p(c_i \mid z_t, x_t) \) maps a sensor reading to occupancy probability. For a LiDAR beam:</p>

    <ul>
        <li>Cells <strong>along the beam</strong> (before the endpoint) are likely <strong>free</strong></li>
        <li>The cell <strong>at the endpoint</strong> is likely <strong>occupied</strong></li>
        <li>Cells <strong>beyond the endpoint</strong> remain <strong>unknown</strong></li>
    </ul>

    <!-- SVG: Occupancy Grid Visualization -->
    <div class="diagram">
        <svg viewBox="0 0 600 280" xmlns="http://www.w3.org/2000/svg" style="width:100%;height:auto;">
            <!-- Grid -->
            <defs>
                <pattern id="grid" width="40" height="40" patternUnits="userSpaceOnUse">
                    <rect width="40" height="40" fill="none" stroke="#ccc" stroke-width="0.5"/>
                </pattern>
            </defs>
            <rect x="40" y="20" width="520" height="240" fill="url(#grid)" stroke="#667eea" stroke-width="2" rx="4"/>
            <!-- Free cells (green) along ray -->
            <rect x="80" y="140" width="40" height="40" fill="#c8e6c9" stroke="#ccc" stroke-width="0.5"/>
            <rect x="120" y="140" width="40" height="40" fill="#c8e6c9" stroke="#ccc" stroke-width="0.5"/>
            <rect x="160" y="140" width="40" height="40" fill="#a5d6a7" stroke="#ccc" stroke-width="0.5"/>
            <rect x="200" y="140" width="40" height="40" fill="#a5d6a7" stroke="#ccc" stroke-width="0.5"/>
            <rect x="240" y="140" width="40" height="40" fill="#81c784" stroke="#ccc" stroke-width="0.5"/>
            <rect x="280" y="140" width="40" height="40" fill="#81c784" stroke="#ccc" stroke-width="0.5"/>
            <rect x="320" y="140" width="40" height="40" fill="#66bb6a" stroke="#ccc" stroke-width="0.5"/>
            <!-- Occupied cell (red) -->
            <rect x="360" y="140" width="40" height="40" fill="#ef5350" stroke="#ccc" stroke-width="0.5"/>
            <!-- Unknown cells (grey) beyond -->
            <rect x="400" y="140" width="40" height="40" fill="#e0e0e0" stroke="#ccc" stroke-width="0.5"/>
            <rect x="440" y="140" width="40" height="40" fill="#e0e0e0" stroke="#ccc" stroke-width="0.5"/>
            <rect x="480" y="140" width="40" height="40" fill="#e0e0e0" stroke="#ccc" stroke-width="0.5"/>
            <!-- Robot -->
            <circle cx="60" cy="160" r="14" fill="#667eea"/>
            <text x="60" y="165" text-anchor="middle" fill="white" font-size="10" font-weight="bold">R</text>
            <!-- Ray line -->
            <line x1="74" y1="160" x2="380" y2="160" stroke="#667eea" stroke-width="2" stroke-dasharray="6,3"/>
            <!-- Legend -->
            <rect x="80" y="230" width="16" height="16" fill="#81c784"/>
            <text x="102" y="243" font-size="11" fill="#333">Free</text>
            <rect x="160" y="230" width="16" height="16" fill="#ef5350"/>
            <text x="182" y="243" font-size="11" fill="#333">Occupied</text>
            <rect x="260" y="230" width="16" height="16" fill="#e0e0e0"/>
            <text x="282" y="243" font-size="11" fill="#333">Unknown</text>
            <circle cx="370" cy="238" r="8" fill="#667eea"/>
            <text x="384" y="243" font-size="11" fill="#333">Robot</text>
        </svg>
    </div>

    <div class="tip">
        Typical grid resolutions for indoor SLAM range from 1 cm to 10 cm per cell. Finer resolution gives more detail but uses more memory: a 100 m x 100 m map at 5 cm resolution requires 4 million cells.
    </div>
</div>

<!-- ==================== PAGE 5: LOG-ODDS REPRESENTATION ==================== -->
<div class="page">
    <h2>Log-Odds Representation</h2>

    <p>Direct probability updates are computationally expensive because they require normalisation. The <strong>log-odds</strong> representation transforms the Bayesian update into simple addition.</p>

    <h3>Definition</h3>

    <p>The log-odds (or logit) of occupancy for cell \( c_i \) is:</p>

    <div class="math">
        \[ l(c_i) = \log \frac{p(c_i)}{1 - p(c_i)} \]
    </div>

    <p>The inverse mapping back to probability is the sigmoid (logistic) function:</p>

    <div class="math">
        \[ p(c_i) = \frac{1}{1 + e^{-l(c_i)}} = \sigma(l(c_i)) \]
    </div>

    <h3>Update Rule</h3>

    <p>Using Bayes' rule in log-odds form, the recursive update becomes a simple <strong>addition</strong>:</p>

    <div class="math">
        \[ l_{t}(c_i) = l_{t-1}(c_i) + \underbrace{\log \frac{p(c_i \mid z_t, x_t)}{1 - p(c_i \mid z_t, x_t)}}_{\text{inverse sensor model (log-odds)}} - \underbrace{\log \frac{p_0(c_i)}{1 - p_0(c_i)}}_{l_0 \text{ (prior)}} \]
    </div>

    <p>With a uniform prior \( p_0 = 0.5 \), we have \( l_0 = 0 \), simplifying to:</p>

    <div class="math">
        \[ \boxed{l_{t}(c_i) = l_{t-1}(c_i) + l_{\text{sensor}}(c_i)} \]
    </div>

    <h3>Advantages of Log-Odds</h3>

    <div class="two-column">
        <div class="info-box">
            <h4>Computational Efficiency</h4>
            <p>Updates reduce to integer addition. No division or normalisation is required.</p>
        </div>
        <div class="info-box">
            <h4>Numerical Stability</h4>
            <p>Probabilities near 0 or 1 cause floating-point issues. Log-odds maps these to \( -\infty \) and \( +\infty \), which are well-handled by clamping.</p>
        </div>
    </div>

    <h3>Numerical Example</h3>

    <div class="example-box">
        <h4>Example: Updating a Cell</h4>
        <p>Suppose a cell currently has \( p(c_i) = 0.5 \) (unknown), so \( l_0 = 0 \).</p>
        <p>The sensor model returns \( p(c_i \mid z_t, x_t) = 0.9 \) (likely occupied).</p>
        <p>The log-odds measurement is:</p>
        <div class="math">
            \[ l_{\text{sensor}} = \log \frac{0.9}{0.1} = \log 9 \approx 2.197 \]
        </div>
        <p>After update: \( l_1 = 0 + 2.197 = 2.197 \)</p>
        <p>Converting back: \( p(c_i) = \sigma(2.197) = \frac{1}{1+e^{-2.197}} \approx 0.9 \)</p>
        <p><strong>After a second identical observation:</strong></p>
        <p>\( l_2 = 2.197 + 2.197 = 4.394 \implies p(c_i) = \sigma(4.394) \approx 0.988 \)</p>
        <p>The cell becomes increasingly certain of occupation with repeated evidence.</p>
    </div>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1635070041078-e363dbe005cb?w=800&h=300&fit=crop" alt="Mathematical concepts visualization">
        <p class="image-caption">Log-odds transforms multiplicative Bayesian updates into efficient additive operations</p>
    </div>

    <div class="tip">
        In practice, clamp log-odds values to a range like \([-10, 10]\) to prevent cells from becoming irreversibly locked to occupied or free states.
    </div>
</div>

<!-- ==================== PAGE 6: BRESENHAM'S RAY CASTING ==================== -->
<div class="page">
    <h2>Bresenham's Ray Casting</h2>

    <p>To update an occupancy grid with a LiDAR scan, we must determine which cells each laser beam passes through. <strong>Bresenham's line algorithm</strong> efficiently traces a ray on a discrete grid without floating-point arithmetic.</p>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1620712943543-bcc4688e7485?w=800&h=300&fit=crop" alt="Digital grid and line tracing concept">
        <p class="image-caption">Ray casting on a discrete grid determines which cells are traversed by each LiDAR beam</p>
    </div>

    <h3>Algorithm Overview</h3>

    <ol>
        <li>Convert the robot position and beam endpoint from world coordinates to grid cell indices</li>
        <li>Use Bresenham's algorithm to enumerate all cells between start and end</li>
        <li>Mark traversed cells as <strong>free</strong> (decrease log-odds)</li>
        <li>Mark the endpoint cell as <strong>occupied</strong> (increase log-odds)</li>
        <li>Leave cells beyond the endpoint <strong>unchanged</strong></li>
    </ol>

    <!-- SVG: Bresenham Ray through Grid -->
    <div class="diagram">
        <svg viewBox="0 0 520 340" xmlns="http://www.w3.org/2000/svg" style="width:100%;height:auto;">
            <!-- Grid lines -->
            <g stroke="#ddd" stroke-width="0.5">
                <line x1="40" y1="20" x2="40" y2="300"/><line x1="80" y1="20" x2="80" y2="300"/>
                <line x1="120" y1="20" x2="120" y2="300"/><line x1="160" y1="20" x2="160" y2="300"/>
                <line x1="200" y1="20" x2="200" y2="300"/><line x1="240" y1="20" x2="240" y2="300"/>
                <line x1="280" y1="20" x2="280" y2="300"/><line x1="320" y1="20" x2="320" y2="300"/>
                <line x1="360" y1="20" x2="360" y2="300"/><line x1="400" y1="20" x2="400" y2="300"/>
                <line x1="440" y1="20" x2="440" y2="300"/><line x1="480" y1="20" x2="480" y2="300"/>
                <line x1="40" y1="20" x2="480" y2="20"/><line x1="40" y1="60" x2="480" y2="60"/>
                <line x1="40" y1="100" x2="480" y2="100"/><line x1="40" y1="140" x2="480" y2="140"/>
                <line x1="40" y1="180" x2="480" y2="180"/><line x1="40" y1="220" x2="480" y2="220"/>
                <line x1="40" y1="260" x2="480" y2="260"/><line x1="40" y1="300" x2="480" y2="300"/>
            </g>
            <!-- Bresenham cells highlighted -->
            <rect x="40" y="260" width="40" height="40" fill="#c8e6c9" opacity="0.8"/>
            <rect x="80" y="220" width="40" height="40" fill="#a5d6a7" opacity="0.8"/>
            <rect x="120" y="220" width="40" height="40" fill="#a5d6a7" opacity="0.8"/>
            <rect x="160" y="180" width="40" height="40" fill="#81c784" opacity="0.8"/>
            <rect x="200" y="180" width="40" height="40" fill="#81c784" opacity="0.8"/>
            <rect x="240" y="140" width="40" height="40" fill="#66bb6a" opacity="0.8"/>
            <rect x="280" y="140" width="40" height="40" fill="#66bb6a" opacity="0.8"/>
            <rect x="320" y="100" width="40" height="40" fill="#43a047" opacity="0.8"/>
            <!-- Endpoint (occupied) -->
            <rect x="360" y="60" width="40" height="40" fill="#ef5350" opacity="0.9"/>
            <!-- Ray line -->
            <line x1="60" y1="280" x2="380" y2="80" stroke="#667eea" stroke-width="2.5" stroke-dasharray="8,4"/>
            <!-- Robot marker -->
            <circle cx="60" cy="280" r="10" fill="#667eea"/>
            <text x="60" y="284" text-anchor="middle" fill="white" font-size="9" font-weight="bold">R</text>
            <!-- Endpoint marker -->
            <circle cx="380" cy="80" r="6" fill="#d32f2f"/>
            <!-- Labels -->
            <text x="60" y="320" text-anchor="middle" font-size="11" fill="#333">Start (robot)</text>
            <text x="380" y="55" text-anchor="middle" font-size="11" fill="#d32f2f" font-weight="bold">Hit point</text>
            <text x="200" y="335" text-anchor="middle" font-size="11" fill="#4CAF50">Free cells (log-odds decreased)</text>
        </svg>
    </div>

    <h3>Python Implementation</h3>

    <pre>
def bresenham(x0, y0, x1, y1):
    """Yield grid cells along the line from (x0,y0) to (x1,y1)."""
    dx = abs(x1 - x0)
    dy = abs(y1 - y0)
    sx = 1 if x0 &lt; x1 else -1
    sy = 1 if y0 &lt; y1 else -1
    err = dx - dy
    while True:
        yield (x0, y0)
        if x0 == x1 and y0 == y1:
            break
        e2 = 2 * err
        if e2 > -dy:
            err -= dy
            x0 += sx
        if e2 &lt; dx:
            err += dx
            y0 += sy

def update_grid(grid, robot_cell, endpoint_cell, l_free, l_occ):
    cells = list(bresenham(*robot_cell, *endpoint_cell))
    for cell in cells[:-1]:       # All but last: free
        grid[cell] += l_free      # l_free is negative (e.g., -0.4)
    grid[cells[-1]] += l_occ      # Last cell: occupied (e.g., +0.85)
    </pre>
</div>

<!-- ==================== PAGE 7: SCAN MATCHING / ICP ==================== -->
<div class="page">
    <h2>Scan Matching: Iterative Closest Point (ICP)</h2>

    <p>Scan matching aligns consecutive LiDAR scans to estimate the relative motion between them. The <strong>Iterative Closest Point (ICP)</strong> algorithm is the most widely used approach.</p>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1581091226825-a6a2a5aee158?w=800&h=300&fit=crop" alt="Point cloud alignment concept">
        <p class="image-caption">ICP iteratively aligns two point clouds by finding correspondences and minimising distances</p>
    </div>

    <h3>ICP Algorithm Steps</h3>

    <div class="flow-step">1. Initial guess for transformation \( T^{(0)} \) (e.g., from odometry)</div>
    <div class="arrow">&darr;</div>
    <div class="flow-step">2. Transform source scan by current estimate \( T^{(k)} \)</div>
    <div class="arrow">&darr;</div>
    <div class="flow-step">3. Find closest point correspondences between transformed source and target</div>
    <div class="arrow">&darr;</div>
    <div class="flow-step">4. Compute optimal \( T^{(k+1)} \) that minimises correspondence distances</div>
    <div class="arrow">&darr;</div>
    <div class="flow-step">5. Check convergence: if \( \|T^{(k+1)} - T^{(k)}\| &lt; \epsilon \), stop; else go to step 2</div>

    <h3>Variants</h3>

    <div class="two-column">
        <div class="sensor-card">
            <h4>Point-to-Point ICP</h4>
            <p>Minimises the sum of squared distances between corresponding points.</p>
            <div class="math">
                \[ E = \sum_{i} \| R p_i + t - q_i \|^2 \]
            </div>
            <p><strong>where:</strong></p>
            <ul>
                <li>\( R \) — rotation matrix</li>
                <li>\( t \) — translation vector</li>
                <li>\( p_i \) — source point from the current scan</li>
                <li>\( q_i \) — closest corresponding point in the reference scan</li>
            </ul>
            <p><em>Simple but can converge slowly on smooth surfaces.</em></p>
        </div>
        <div class="sensor-card">
            <h4>Point-to-Line ICP</h4>
            <p>Minimises the distance from each point to the <em>tangent line</em> of the corresponding surface.</p>
            <div class="math">
                \[ E = \sum_{i} \left( (R p_i + t - q_i) \cdot \hat{n}_i \right)^2 \]
            </div>
            <p><em>Faster convergence; better for structured environments.</em></p>
        </div>
    </div>

    <!-- SVG: ICP Iteration Diagram -->
    <div class="diagram">
        <svg viewBox="0 0 650 200" xmlns="http://www.w3.org/2000/svg" style="width:100%;height:auto;">
            <!-- Iteration 0 -->
            <text x="75" y="15" text-anchor="middle" font-size="12" fill="#764ba2" font-weight="bold">Iteration 0</text>
            <!-- Target points (blue) -->
            <circle cx="30" cy="80" r="5" fill="#667eea"/><circle cx="50" cy="55" r="5" fill="#667eea"/>
            <circle cx="80" cy="40" r="5" fill="#667eea"/><circle cx="110" cy="50" r="5" fill="#667eea"/>
            <circle cx="130" cy="75" r="5" fill="#667eea"/>
            <!-- Source points (red, misaligned) -->
            <circle cx="55" cy="120" r="5" fill="#ef5350"/><circle cx="70" cy="95" r="5" fill="#ef5350"/>
            <circle cx="100" cy="78" r="5" fill="#ef5350"/><circle cx="125" cy="90" r="5" fill="#ef5350"/>
            <circle cx="140" cy="115" r="5" fill="#ef5350"/>
            <!-- Arrow -->
            <text x="185" y="85" font-size="28" fill="#764ba2">&rarr;</text>
            <!-- Iteration 2 -->
            <text x="290" y="15" text-anchor="middle" font-size="12" fill="#764ba2" font-weight="bold">Iteration 2</text>
            <circle cx="240" cy="80" r="5" fill="#667eea"/><circle cx="260" cy="55" r="5" fill="#667eea"/>
            <circle cx="290" cy="40" r="5" fill="#667eea"/><circle cx="320" cy="50" r="5" fill="#667eea"/>
            <circle cx="340" cy="75" r="5" fill="#667eea"/>
            <circle cx="248" cy="95" r="5" fill="#ef5350"/><circle cx="265" cy="68" r="5" fill="#ef5350"/>
            <circle cx="295" cy="52" r="5" fill="#ef5350"/><circle cx="322" cy="62" r="5" fill="#ef5350"/>
            <circle cx="340" cy="88" r="5" fill="#ef5350"/>
            <!-- Arrow -->
            <text x="400" y="85" font-size="28" fill="#764ba2">&rarr;</text>
            <!-- Converged -->
            <text x="530" y="15" text-anchor="middle" font-size="12" fill="#4CAF50" font-weight="bold">Converged</text>
            <circle cx="465" cy="80" r="5" fill="#667eea"/><circle cx="485" cy="55" r="5" fill="#667eea"/>
            <circle cx="515" cy="40" r="5" fill="#667eea"/><circle cx="545" cy="50" r="5" fill="#667eea"/>
            <circle cx="565" cy="75" r="5" fill="#667eea"/>
            <circle cx="466" cy="82" r="5" fill="#ef5350" opacity="0.7"/><circle cx="486" cy="57" r="5" fill="#ef5350" opacity="0.7"/>
            <circle cx="516" cy="42" r="5" fill="#ef5350" opacity="0.7"/><circle cx="546" cy="52" r="5" fill="#ef5350" opacity="0.7"/>
            <circle cx="566" cy="77" r="5" fill="#ef5350" opacity="0.7"/>
            <!-- Legend -->
            <circle cx="200" cy="175" r="5" fill="#667eea"/>
            <text x="212" y="179" font-size="11" fill="#333">Target scan</text>
            <circle cx="330" cy="175" r="5" fill="#ef5350"/>
            <text x="342" y="179" font-size="11" fill="#333">Source scan (being aligned)</text>
        </svg>
    </div>

    <div class="warning-box">
        <strong>ICP Pitfall:</strong> ICP converges to a <em>local</em> minimum. A poor initial guess (e.g., large motion between scans) can cause ICP to converge to the wrong alignment. Always provide a good initial estimate from odometry or IMU.
    </div>
</div>

<!-- ==================== PAGE 8: ICP MATHEMATICS ==================== -->
<div class="page">
    <h2>ICP Mathematics: SVD-Based Solution</h2>

    <p>Given a set of correspondences \(\{(p_i, q_i)\}_{i=1}^N\), we seek the rotation \(R\) and translation \(t\) that minimise:</p>

    <div class="math">
        \[ E(R, t) = \sum_{i=1}^{N} \| q_i - (R p_i + t) \|^2 \]
    </div>

    <h3>Step 1: Compute Centroids</h3>

    <div class="math">
        \[ \bar{p} = \frac{1}{N} \sum_{i=1}^{N} p_i, \qquad \bar{q} = \frac{1}{N} \sum_{i=1}^{N} q_i \]
    </div>

    <h3>Step 2: Centre the Points</h3>

    <div class="math">
        \[ p_i' = p_i - \bar{p}, \qquad q_i' = q_i - \bar{q} \]
    </div>

    <h3>Step 3: Cross-Covariance Matrix</h3>

    <div class="math">
        \[ H = \sum_{i=1}^{N} p_i' \, q_i'^{\!\top} \in \mathbb{R}^{2 \times 2} \]
    </div>

    <h3>Step 4: SVD Decomposition</h3>

    <div class="math">
        \[ H = U \Sigma V^{\top} \]
    </div>

    <h3>Step 5: Optimal Rotation and Translation</h3>

    <div class="math">
        \[ \boxed{R^* = V U^{\top}} \]
    </div>

    <div class="math">
        \[ \boxed{t^* = \bar{q} - R^* \bar{p}} \]
    </div>

    <div class="info-box">
        <strong>Note:</strong> In 2D, the rotation matrix has a single degree of freedom (angle \(\theta\)):
        \[ R = \begin{pmatrix} \cos\theta & -\sin\theta \\ \sin\theta & \cos\theta \end{pmatrix} \]
        The SVD solution automatically recovers this angle. Ensure \(\det(R^*) = +1\); if \(\det(VU^\top) = -1\), negate the last column of \(V\).
    </div>

    <h3>Python SVD Solution</h3>

    <pre>
import numpy as np

def icp_svd(source, target):
    """Compute R, t aligning source to target (Nx2 arrays)."""
    # Centroids
    p_bar = source.mean(axis=0)
    q_bar = target.mean(axis=0)
    # Centre
    P = source - p_bar
    Q = target - q_bar
    # Cross-covariance
    H = P.T @ Q  # 2x2
    U, S, Vt = np.linalg.svd(H)
    V = Vt.T
    # Ensure proper rotation (det = +1)
    d = np.linalg.det(V @ U.T)
    D = np.diag([1, d])
    R = V @ D @ U.T
    t = q_bar - R @ p_bar
    return R, t
    </pre>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1509228468518-180dd4864904?w=800&h=250&fit=crop" alt="Mathematical computation">
        <p class="image-caption">The SVD decomposition provides a closed-form optimal solution for the rigid alignment problem</p>
    </div>

    <div class="tip">
        For 2D scans, you can also solve for \(\theta\) and \(t\) directly using the Kabsch algorithm or atan2-based closed forms, which avoids the SVD entirely.
    </div>
</div>

<!-- ==================== PAGE 9: CORRELATIVE SCAN MATCHING ==================== -->
<div class="page">
    <h2>Correlative Scan Matching</h2>

    <p>Unlike ICP, which iteratively refines correspondences, <strong>correlative scan matching</strong> evaluates the alignment score over a search window to find the best match. This approach is more robust to poor initial guesses.</p>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800&h=300&fit=crop" alt="Search and optimization concept">
        <p class="image-caption">Correlative methods exhaustively search a discretised pose space for optimal alignment</p>
    </div>

    <h3>Brute-Force Approach</h3>

    <p>Define a search window \(\mathcal{W}\) around the initial pose estimate \((x_0, y_0, \theta_0)\):</p>

    <div class="math">
        \[ \xi^* = \arg\max_{(x, y, \theta) \in \mathcal{W}} \sum_{k=1}^{K} M_{\text{smooth}}\left( T(x, y, \theta) \cdot s_k \right) \]
    </div>

    <p>where \(M_{\text{smooth}}\) is a smoothed version of the existing map and \(s_k\) are the scan points.</p>

    <h3>Multi-Resolution Search</h3>

    <p>To reduce computation, use a <strong>coarse-to-fine</strong> strategy:</p>

    <ol>
        <li>Create map pyramids at multiple resolutions (e.g., 16x, 8x, 4x, 2x, 1x)</li>
        <li>At the coarsest level, search the full window</li>
        <li>At each finer level, search only around the best candidates from the previous level</li>
        <li>This is used by <strong>Google Cartographer</strong> for real-time performance</li>
    </ol>

    <!-- SVG: Multi-resolution grid search -->
    <div class="diagram">
        <svg viewBox="0 0 650 220" xmlns="http://www.w3.org/2000/svg" style="width:100%;height:auto;">
            <!-- Coarse grid -->
            <text x="80" y="20" text-anchor="middle" font-size="12" fill="#764ba2" font-weight="bold">Coarse (4x)</text>
            <rect x="20" y="30" width="120" height="120" fill="none" stroke="#667eea" stroke-width="2"/>
            <line x1="50" y1="30" x2="50" y2="150" stroke="#ccc" stroke-width="1"/>
            <line x1="80" y1="30" x2="80" y2="150" stroke="#ccc" stroke-width="1"/>
            <line x1="110" y1="30" x2="110" y2="150" stroke="#ccc" stroke-width="1"/>
            <line x1="20" y1="60" x2="140" y2="60" stroke="#ccc" stroke-width="1"/>
            <line x1="20" y1="90" x2="140" y2="90" stroke="#ccc" stroke-width="1"/>
            <line x1="20" y1="120" x2="140" y2="120" stroke="#ccc" stroke-width="1"/>
            <rect x="80" y="60" width="30" height="30" fill="#667eea" opacity="0.3"/>
            <text x="95" y="80" text-anchor="middle" font-size="10" fill="#667eea" font-weight="bold">best</text>
            <!-- Arrow -->
            <text x="170" y="95" font-size="24" fill="#764ba2">&rarr;</text>
            <!-- Medium grid -->
            <text x="280" y="20" text-anchor="middle" font-size="12" fill="#764ba2" font-weight="bold">Medium (2x)</text>
            <rect x="220" y="30" width="120" height="120" fill="none" stroke="#667eea" stroke-width="2"/>
            <g stroke="#ccc" stroke-width="0.5">
                <line x1="235" y1="30" x2="235" y2="150"/><line x1="250" y1="30" x2="250" y2="150"/>
                <line x1="265" y1="30" x2="265" y2="150"/><line x1="280" y1="30" x2="280" y2="150"/>
                <line x1="295" y1="30" x2="295" y2="150"/><line x1="310" y1="30" x2="310" y2="150"/>
                <line x1="325" y1="30" x2="325" y2="150"/>
                <line x1="220" y1="45" x2="340" y2="45"/><line x1="220" y1="60" x2="340" y2="60"/>
                <line x1="220" y1="75" x2="340" y2="75"/><line x1="220" y1="90" x2="340" y2="90"/>
                <line x1="220" y1="105" x2="340" y2="105"/><line x1="220" y1="120" x2="340" y2="120"/>
                <line x1="220" y1="135" x2="340" y2="135"/>
            </g>
            <rect x="280" y="75" width="15" height="15" fill="#667eea" opacity="0.4"/>
            <!-- Arrow -->
            <text x="370" y="95" font-size="24" fill="#764ba2">&rarr;</text>
            <!-- Fine grid -->
            <text x="490" y="20" text-anchor="middle" font-size="12" fill="#4CAF50" font-weight="bold">Fine (1x)</text>
            <rect x="430" y="30" width="120" height="120" fill="none" stroke="#4CAF50" stroke-width="2"/>
            <g stroke="#eee" stroke-width="0.3">
                <line x1="437.5" y1="30" x2="437.5" y2="150"/><line x1="445" y1="30" x2="445" y2="150"/>
                <line x1="452.5" y1="30" x2="452.5" y2="150"/><line x1="460" y1="30" x2="460" y2="150"/>
                <line x1="467.5" y1="30" x2="467.5" y2="150"/><line x1="475" y1="30" x2="475" y2="150"/>
                <line x1="482.5" y1="30" x2="482.5" y2="150"/><line x1="490" y1="30" x2="490" y2="150"/>
                <line x1="497.5" y1="30" x2="497.5" y2="150"/><line x1="505" y1="30" x2="505" y2="150"/>
                <line x1="512.5" y1="30" x2="512.5" y2="150"/><line x1="520" y1="30" x2="520" y2="150"/>
                <line x1="527.5" y1="30" x2="527.5" y2="150"/><line x1="535" y1="30" x2="535" y2="150"/>
                <line x1="542.5" y1="30" x2="542.5" y2="150"/>
            </g>
            <circle cx="492" cy="88" r="5" fill="#4CAF50"/>
            <text x="492" y="115" text-anchor="middle" font-size="10" fill="#4CAF50" font-weight="bold">optimal</text>
            <!-- Label -->
            <text x="325" y="195" text-anchor="middle" font-size="11" fill="#333" font-style="italic">Search region narrows at each resolution level</text>
        </svg>
    </div>

    <h3>Comparison: ICP vs Correlative</h3>

    <table>
        <tr><th>Aspect</th><th>ICP</th><th>Correlative</th></tr>
        <tr><td>Initial guess sensitivity</td><td>High</td><td>Low (searches window)</td></tr>
        <tr><td>Computation</td><td>Fast per iteration</td><td>Proportional to search window</td></tr>
        <tr><td>Local minima</td><td>Prone</td><td>Finds global optimum in window</td></tr>
        <tr><td>Used in</td><td>Many classic SLAM systems</td><td>Google Cartographer</td></tr>
    </table>

    <div class="activity-box">
        <h3>Activity 1: Scan Matching Comparison</h3>
        <p><strong>Objective:</strong> Compare ICP and correlative scan matching on synthetic data.</p>
        <ol>
            <li>Generate a 2D point cloud of 100 points forming an L-shaped corridor</li>
            <li>Create a second scan by applying a known rotation (15 degrees) and translation (0.3 m, 0.2 m)</li>
            <li>Add Gaussian noise (\(\sigma = 0.02\) m) to both scans</li>
            <li>Implement point-to-point ICP and recover the transformation</li>
            <li>Compare the recovered transformation to the ground truth</li>
            <li>Test with increasingly poor initial guesses &mdash; at what point does ICP fail?</li>
        </ol>
    </div>
</div>

<!-- ==================== PAGE 10: MOTION MODELS FOR SLAM ==================== -->
<div class="page">
    <h2>Motion Models for SLAM</h2>

    <p>SLAM algorithms require a <strong>motion model</strong> (also called the process model) that predicts the robot's next pose given a control input. Since real motion is noisy, we use probabilistic models.</p>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1563207153-f403bf289096?w=800&h=300&fit=crop" alt="Robot motion and uncertainty">
        <p class="image-caption">Odometry measurements are subject to noise, creating pose uncertainty that grows over time</p>
    </div>

    <h3>Odometry Motion Model</h3>

    <p>The odometry model decomposes the motion between two poses into three steps: an initial rotation \(\delta_{\text{rot1}}\), a translation \(\delta_{\text{trans}}\), and a final rotation \(\delta_{\text{rot2}}\).</p>

    <div class="math">
        \[ \delta_{\text{rot1}} = \text{atan2}(y' - y, \; x' - x) - \theta \]
        \[ \delta_{\text{trans}} = \sqrt{(x' - x)^2 + (y' - y)^2} \]
        \[ \delta_{\text{rot2}} = \theta' - \theta - \delta_{\text{rot1}} \]
    </div>

    <h3>Probabilistic (Noisy) Model</h3>

    <p>We add zero-mean Gaussian noise to each component:</p>

    <div class="math">
        \[ \hat{\delta}_{\text{rot1}} = \delta_{\text{rot1}} + \epsilon_1, \quad \epsilon_1 \sim \mathcal{N}(0, \; \alpha_1 |\delta_{\text{rot1}}| + \alpha_2 \delta_{\text{trans}}) \]
        \[ \hat{\delta}_{\text{trans}} = \delta_{\text{trans}} + \epsilon_2, \quad \epsilon_2 \sim \mathcal{N}(0, \; \alpha_3 \delta_{\text{trans}} + \alpha_4 (|\delta_{\text{rot1}}| + |\delta_{\text{rot2}}|)) \]
        \[ \hat{\delta}_{\text{rot2}} = \delta_{\text{rot2}} + \epsilon_3, \quad \epsilon_3 \sim \mathcal{N}(0, \; \alpha_1 |\delta_{\text{rot2}}| + \alpha_2 \delta_{\text{trans}}) \]
    </div>

    <p>where \(\alpha_1, \alpha_2, \alpha_3, \alpha_4\) are noise parameters characterising the robot.</p>

    <!-- SVG: Motion uncertainty growing -->
    <div class="diagram">
        <svg viewBox="0 0 600 200" xmlns="http://www.w3.org/2000/svg" style="width:100%;height:auto;">
            <!-- Robot trajectory -->
            <line x1="60" y1="120" x2="200" y2="120" stroke="#667eea" stroke-width="2"/>
            <line x1="200" y1="120" x2="350" y2="80" stroke="#667eea" stroke-width="2"/>
            <line x1="350" y1="80" x2="520" y2="90" stroke="#667eea" stroke-width="2"/>
            <!-- Uncertainty ellipses (growing) -->
            <ellipse cx="200" cy="120" rx="15" ry="10" fill="#667eea" opacity="0.15" stroke="#667eea" stroke-width="1"/>
            <ellipse cx="350" cy="80" rx="30" ry="20" fill="#667eea" opacity="0.12" stroke="#667eea" stroke-width="1"/>
            <ellipse cx="520" cy="90" rx="50" ry="35" fill="#667eea" opacity="0.1" stroke="#667eea" stroke-width="1"/>
            <!-- Robot markers -->
            <circle cx="60" cy="120" r="8" fill="#4CAF50"/>
            <text x="60" y="124" text-anchor="middle" fill="white" font-size="8" font-weight="bold">t0</text>
            <circle cx="200" cy="120" r="6" fill="#667eea"/>
            <text x="200" y="145" text-anchor="middle" font-size="10" fill="#333">t1</text>
            <circle cx="350" cy="80" r="6" fill="#667eea"/>
            <text x="350" y="60" text-anchor="middle" font-size="10" fill="#333">t2</text>
            <circle cx="520" cy="90" r="6" fill="#ef5350"/>
            <text x="520" y="70" text-anchor="middle" font-size="10" fill="#ef5350">t3</text>
            <!-- Label -->
            <text x="300" y="190" text-anchor="middle" font-size="11" fill="#764ba2" font-style="italic">Pose uncertainty grows without corrections (sensor updates)</text>
        </svg>
    </div>

    <h3>The Velocity Motion Model</h3>

    <p>An alternative parametrisation uses translational velocity \(v\) and rotational velocity \(\omega\):</p>

    <div class="math">
        \[ \begin{pmatrix} x' \\ y' \\ \theta' \end{pmatrix} = \begin{pmatrix} x \\ y \\ \theta \end{pmatrix} + \begin{pmatrix} -\frac{v}{\omega}\sin\theta + \frac{v}{\omega}\sin(\theta + \omega \Delta t) \\ \frac{v}{\omega}\cos\theta - \frac{v}{\omega}\cos(\theta + \omega \Delta t) \\ \omega \Delta t \end{pmatrix} \]
    </div>

    <div class="tip">
        The odometry model is preferred when wheel encoder data is available; the velocity model is used when control commands (v, omega) are the inputs. Both produce comparable results when properly tuned.
    </div>

    <div class="activity-box">
        <h3>Activity 2: Simulating Motion Model Noise</h3>
        <p><strong>Objective:</strong> Visualise how odometry noise affects pose estimation.</p>
        <ol>
            <li>Implement the probabilistic odometry model in Python</li>
            <li>Simulate a robot driving in a 4 m x 4 m square path (4 straight segments + 4 turns)</li>
            <li>Sample 200 trajectories with noise parameters \(\alpha_1 = 0.05, \alpha_2 = 0.01, \alpha_3 = 0.05, \alpha_4 = 0.01\)</li>
            <li>Plot all trajectories overlaid &mdash; observe how uncertainty grows</li>
            <li>Increase noise parameters by 5x and observe the effect</li>
        </ol>
    </div>
</div>

<!-- ==================== PAGE 11: PARTICLE FILTER SLAM ==================== -->
<div class="page">
    <h2>Particle Filter SLAM (FastSLAM)</h2>

    <p>FastSLAM uses a <strong>Rao-Blackwellised particle filter</strong> to factorise the SLAM posterior into a tractable form. Each particle maintains its own pose hypothesis <em>and</em> its own map.</p>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1516110833967-0b5716ca1387?w=800&h=300&fit=crop" alt="Particle distribution concept">
        <p class="image-caption">Particles represent possible robot trajectories; each carries its own map hypothesis</p>
    </div>

    <h3>Rao-Blackwellised Factorisation</h3>

    <p>The key insight is that if the trajectory \(x_{1:t}\) were known, map estimation becomes independent per cell. This allows the factorisation:</p>

    <div class="math">
        \[ p(x_{1:t}, m \mid z_{1:t}, u_{1:t}) = \underbrace{p(x_{1:t} \mid z_{1:t}, u_{1:t})}_{\text{particle filter}} \cdot \underbrace{p(m \mid x_{1:t}, z_{1:t})}_{\text{occupancy grid mapping}} \]
    </div>

    <h3>Algorithm: GMapping (Rao-Blackwellised Particle Filter SLAM)</h3>

    <ol>
        <li><strong>Prediction:</strong> For each particle, sample a new pose from the motion model:
            \( x_t^{[k]} \sim p(x_t \mid x_{t-1}^{[k]}, u_t) \)</li>
        <li><strong>Scan Matching:</strong> Refine each particle's pose using ICP or likelihood maximisation against its map</li>
        <li><strong>Weight Update:</strong> Compute particle weight based on observation likelihood:
            \( w^{[k]} = p(z_t \mid x_t^{[k]}, m^{[k]}) \)</li>
        <li><strong>Resampling:</strong> Resample particles proportional to weights (systematic resampling)</li>
        <li><strong>Map Update:</strong> Each surviving particle updates its occupancy grid with the new scan</li>
    </ol>

    <!-- SVG: Particle cloud diagram -->
    <div class="diagram">
        <svg viewBox="0 0 600 250" xmlns="http://www.w3.org/2000/svg" style="width:100%;height:auto;">
            <!-- Room outline -->
            <rect x="50" y="30" width="500" height="180" fill="none" stroke="#999" stroke-width="2" stroke-dasharray="5,3"/>
            <!-- Obstacles -->
            <rect x="200" y="30" width="20" height="80" fill="#bbb"/>
            <rect x="350" y="130" width="80" height="20" fill="#bbb"/>
            <rect x="400" y="30" width="20" height="60" fill="#bbb"/>
            <!-- Particles (scattered dots) -->
            <circle cx="140" cy="140" r="4" fill="#ef5350" opacity="0.5"/>
            <circle cx="148" cy="135" r="4" fill="#ef5350" opacity="0.5"/>
            <circle cx="135" cy="148" r="4" fill="#ef5350" opacity="0.6"/>
            <circle cx="152" cy="142" r="4" fill="#ef5350" opacity="0.4"/>
            <circle cx="142" cy="130" r="4" fill="#ef5350" opacity="0.7"/>
            <circle cx="155" cy="138" r="4" fill="#ef5350" opacity="0.3"/>
            <circle cx="130" cy="136" r="4" fill="#ef5350" opacity="0.6"/>
            <circle cx="146" cy="150" r="4" fill="#ef5350" opacity="0.4"/>
            <circle cx="138" cy="125" r="4" fill="#ef5350" opacity="0.5"/>
            <circle cx="160" cy="145" r="4" fill="#ef5350" opacity="0.35"/>
            <circle cx="125" cy="142" r="4" fill="#ef5350" opacity="0.45"/>
            <circle cx="150" cy="128" r="4" fill="#ef5350" opacity="0.55"/>
            <circle cx="143" cy="155" r="4" fill="#ef5350" opacity="0.3"/>
            <circle cx="133" cy="132" r="4" fill="#ef5350" opacity="0.65"/>
            <circle cx="158" cy="150" r="4" fill="#ef5350" opacity="0.4"/>
            <!-- Best particle (largest) -->
            <circle cx="143" cy="140" r="7" fill="#4CAF50" stroke="#333" stroke-width="1"/>
            <!-- True position -->
            <circle cx="145" cy="138" r="3" fill="#667eea"/>
            <!-- Labels -->
            <text x="143" y="175" text-anchor="middle" font-size="10" fill="#ef5350">Particle cloud</text>
            <circle cx="400" cy="200" r="5" fill="#4CAF50"/>
            <text x="415" y="204" font-size="10" fill="#333">Highest weight</text>
            <circle cx="260" cy="200" r="4" fill="#ef5350" opacity="0.5"/>
            <text x="275" y="204" font-size="10" fill="#333">Particles</text>
            <circle cx="120" cy="200" r="3" fill="#667eea"/>
            <text x="135" y="204" font-size="10" fill="#333">True pose</text>
        </svg>
    </div>

    <div class="warning-box">
        <strong>Particle Depletion:</strong> After resampling, particles with low weights are eliminated. Over time, diversity decreases. GMapping mitigates this by using an <em>adaptive resampling</em> strategy that only resamples when the effective number of particles \( N_{\text{eff}} = \frac{1}{\sum_k (w^{[k]})^2} \) drops below a threshold.
    </div>

    <div class="info-box">
        <strong>Memory Cost:</strong> Each particle stores a full occupancy grid. With \(N\) particles and a grid of \(W \times H\) cells, memory usage is \( O(N \cdot W \cdot H) \). Typical configurations use 30-80 particles for indoor environments.
    </div>
</div>

<!-- ==================== PAGE 12: GRAPH-BASED SLAM ==================== -->
<div class="page">
    <h2>Graph-Based SLAM</h2>

    <p>Graph-based SLAM formulates the problem as optimising a <strong>pose graph</strong>. Nodes represent robot poses, and edges encode spatial constraints derived from odometry and scan matching.</p>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1558494949-ef010cbdcc31?w=800&h=300&fit=crop" alt="Network graph concept">
        <p class="image-caption">A pose graph encodes the SLAM problem as a network of spatial constraints to be optimised</p>
    </div>

    <h3>Pose Graph Structure</h3>

    <div class="two-column">
        <div class="info-box">
            <h4>Nodes (Vertices)</h4>
            <p>Each node \(x_i = (x, y, \theta)_i\) represents the robot's pose at time step \(i\).</p>
        </div>
        <div class="info-box">
            <h4>Edges (Constraints)</h4>
            <p>Each edge \(e_{ij}\) encodes the relative transformation \(z_{ij}\) between poses \(x_i\) and \(x_j\) with information matrix \(\Omega_{ij}\).</p>
        </div>
    </div>

    <h3>Types of Edges</h3>

    <ul>
        <li><strong>Odometry edges:</strong> Connect consecutive poses \(x_i \to x_{i+1}\) from wheel encoders</li>
        <li><strong>Scan matching edges:</strong> Connect consecutive poses from LiDAR alignment</li>
        <li><strong>Loop closure edges:</strong> Connect non-consecutive poses when the robot revisits a location</li>
    </ul>

    <!-- SVG: Pose Graph with Loop Closure -->
    <div class="diagram">
        <svg viewBox="0 0 600 280" xmlns="http://www.w3.org/2000/svg" style="width:100%;height:auto;">
            <!-- Edges (odometry) -->
            <line x1="80" y1="200" x2="160" y2="200" stroke="#667eea" stroke-width="2"/>
            <line x1="160" y1="200" x2="240" y2="160" stroke="#667eea" stroke-width="2"/>
            <line x1="240" y1="160" x2="320" y2="100" stroke="#667eea" stroke-width="2"/>
            <line x1="320" y1="100" x2="400" y2="80" stroke="#667eea" stroke-width="2"/>
            <line x1="400" y1="80" x2="480" y2="100" stroke="#667eea" stroke-width="2"/>
            <line x1="480" y1="100" x2="520" y2="160" stroke="#667eea" stroke-width="2"/>
            <line x1="520" y1="160" x2="480" y2="220" stroke="#667eea" stroke-width="2"/>
            <line x1="480" y1="220" x2="400" y2="240" stroke="#667eea" stroke-width="2"/>
            <line x1="400" y1="240" x2="300" y2="240" stroke="#667eea" stroke-width="2"/>
            <line x1="300" y1="240" x2="200" y2="230" stroke="#667eea" stroke-width="2"/>
            <line x1="200" y1="230" x2="120" y2="220" stroke="#667eea" stroke-width="2"/>
            <!-- Loop closure edge (red, dashed) -->
            <line x1="120" y1="220" x2="80" y2="200" stroke="#ef5350" stroke-width="3" stroke-dasharray="8,4"/>
            <!-- Nodes -->
            <circle cx="80" cy="200" r="10" fill="#667eea" stroke="#333" stroke-width="1.5"/>
            <text x="80" y="204" text-anchor="middle" fill="white" font-size="8" font-weight="bold">x0</text>
            <circle cx="160" cy="200" r="8" fill="#667eea"/>
            <text x="160" y="204" text-anchor="middle" fill="white" font-size="7">x1</text>
            <circle cx="240" cy="160" r="8" fill="#667eea"/>
            <text x="240" y="164" text-anchor="middle" fill="white" font-size="7">x2</text>
            <circle cx="320" cy="100" r="8" fill="#667eea"/>
            <text x="320" y="104" text-anchor="middle" fill="white" font-size="7">x3</text>
            <circle cx="400" cy="80" r="8" fill="#667eea"/>
            <text x="400" y="84" text-anchor="middle" fill="white" font-size="7">x4</text>
            <circle cx="480" cy="100" r="8" fill="#667eea"/>
            <text x="480" y="104" text-anchor="middle" fill="white" font-size="7">x5</text>
            <circle cx="520" cy="160" r="8" fill="#667eea"/>
            <text x="520" y="164" text-anchor="middle" fill="white" font-size="7">x6</text>
            <circle cx="480" cy="220" r="8" fill="#667eea"/>
            <text x="480" y="224" text-anchor="middle" fill="white" font-size="7">x7</text>
            <circle cx="400" cy="240" r="8" fill="#667eea"/>
            <text x="400" y="244" text-anchor="middle" fill="white" font-size="7">x8</text>
            <circle cx="300" cy="240" r="8" fill="#667eea"/>
            <text x="300" y="244" text-anchor="middle" fill="white" font-size="7">x9</text>
            <circle cx="200" cy="230" r="8" fill="#667eea"/>
            <text x="200" y="234" text-anchor="middle" fill="white" font-size="7">x10</text>
            <circle cx="120" cy="220" r="10" fill="#ef5350" stroke="#333" stroke-width="1.5"/>
            <text x="120" y="224" text-anchor="middle" fill="white" font-size="7">x11</text>
            <!-- Legend -->
            <line x1="160" y1="40" x2="200" y2="40" stroke="#667eea" stroke-width="2"/>
            <text x="210" y="44" font-size="11" fill="#333">Odometry / scan matching edge</text>
            <line x1="160" y1="60" x2="200" y2="60" stroke="#ef5350" stroke-width="3" stroke-dasharray="8,4"/>
            <text x="210" y="64" font-size="11" fill="#333">Loop closure edge</text>
        </svg>
    </div>

    <h3>Optimisation Objective</h3>

    <p>Find poses \(x^*\) that minimise the sum of squared errors over all edges:</p>

    <div class="math">
        \[ x^* = \arg\min_x \sum_{(i,j) \in \mathcal{E}} e_{ij}(x_i, x_j)^\top \; \Omega_{ij} \; e_{ij}(x_i, x_j) \]
    </div>
    <p><strong>where:</strong></p>
    <ul>
        <li>\( x_i, x_j \) — poses (nodes) in the graph</li>
        <li>\( \mathcal{E} \) — set of edges (constraints) connecting poses</li>
        <li>\( e_{ij} \) — error between the observed and predicted relative transformation</li>
        <li>\( \Omega_{ij} \) — information matrix (inverse covariance) weighting the constraint</li>
        <li>\( z_{ij} \) — measured relative transformation between poses \(i\) and \(j\)</li>
        <li>\( \oplus \) — pose composition operator</li>
    </ul>

    <p>The error function is:</p>

    <div class="math">
        \[ e_{ij}(x_i, x_j) = z_{ij}^{-1} \oplus (x_i^{-1} \oplus x_j) \]
    </div>

    <p>This is a nonlinear least squares problem solved iteratively.</p>

    <div class="tip">
        Popular graph optimisation backends include <strong>g2o</strong> (General Graph Optimization), <strong>GTSAM</strong> (Georgia Tech Smoothing and Mapping), and <strong>Ceres Solver</strong> (Google).
    </div>
</div>

<!-- ==================== PAGE 13: GRAPH OPTIMISATION MATH ==================== -->
<div class="page">
    <h2>Graph Optimisation Mathematics</h2>

    <p>Pose graph optimisation is a nonlinear least squares problem. We linearise and solve iteratively using Gauss-Newton or Levenberg-Marquardt methods.</p>

    <h3>Cost Function</h3>

    <p>Let \(\mathbf{x}\) be the stacked vector of all poses. The total cost is:</p>

    <div class="math">
        \[ F(\mathbf{x}) = \sum_{(i,j) \in \mathcal{E}} \underbrace{e_{ij}(\mathbf{x})^\top \, \Omega_{ij} \, e_{ij}(\mathbf{x})}_{F_{ij}} \]
    </div>

    <h3>Linearisation</h3>

    <p>Linearise the error around the current estimate \(\breve{\mathbf{x}}\):</p>

    <div class="math">
        \[ e_{ij}(\breve{\mathbf{x}} + \Delta\mathbf{x}) \approx e_{ij}(\breve{\mathbf{x}}) + J_{ij} \, \Delta\mathbf{x} \]
    </div>

    <p>where \(J_{ij} = \frac{\partial e_{ij}}{\partial \mathbf{x}} \Big|_{\breve{\mathbf{x}}}\) is the Jacobian of the error function.</p>

    <h3>Substituting into the Cost Function</h3>

    <div class="math">
        \[ F(\breve{\mathbf{x}} + \Delta\mathbf{x}) \approx \sum_{ij} (e_{ij} + J_{ij} \Delta\mathbf{x})^\top \Omega_{ij} (e_{ij} + J_{ij} \Delta\mathbf{x}) \]
    </div>

    <p>Expanding and taking the derivative with respect to \(\Delta\mathbf{x}\) and setting to zero gives the <strong>normal equations</strong>:</p>

    <div class="math">
        \[ \boxed{H \, \Delta\mathbf{x}^* = -\mathbf{b}} \]
    </div>

    <p>where:</p>

    <div class="math">
        \[ H = \sum_{ij} J_{ij}^\top \, \Omega_{ij} \, J_{ij}, \qquad \mathbf{b} = \sum_{ij} J_{ij}^\top \, \Omega_{ij} \, e_{ij} \]
    </div>

    <h3>Gauss-Newton vs Levenberg-Marquardt</h3>

    <table>
        <tr><th>Method</th><th>Update Rule</th><th>Properties</th></tr>
        <tr>
            <td><strong>Gauss-Newton</strong></td>
            <td>\( H \, \Delta\mathbf{x} = -\mathbf{b} \)</td>
            <td>Fast convergence near optimum; may diverge far from it</td>
        </tr>
        <tr>
            <td><strong>Levenberg-Marquardt</strong></td>
            <td>\( (H + \lambda I) \, \Delta\mathbf{x} = -\mathbf{b} \)</td>
            <td>Damped; interpolates between Gauss-Newton (\(\lambda \to 0\)) and gradient descent (\(\lambda \to \infty\))</td>
        </tr>
    </table>

    <div class="info-box">
        <strong>Sparsity:</strong> The Hessian \(H\) is <strong>sparse</strong> because each edge only involves two poses. Sparse Cholesky factorisation or conjugate gradient methods exploit this structure to solve large pose graphs efficiently. A graph with 10,000 poses can be optimised in milliseconds.
    </div>

    <h3>Iterative Update</h3>

    <div class="flow-step">1. Linearise error around current estimate: compute \(J_{ij}\) and \(e_{ij}\)</div>
    <div class="arrow">&darr;</div>
    <div class="flow-step">2. Build \(H\) and \(\mathbf{b}\) (sparse assembly)</div>
    <div class="arrow">&darr;</div>
    <div class="flow-step">3. Solve \(H \Delta\mathbf{x} = -\mathbf{b}\) using sparse Cholesky</div>
    <div class="arrow">&darr;</div>
    <div class="flow-step">4. Update: \(\mathbf{x} \leftarrow \mathbf{x} \oplus \Delta\mathbf{x}\)</div>
    <div class="arrow">&darr;</div>
    <div class="flow-step">5. Repeat until \(\|\Delta\mathbf{x}\| &lt; \epsilon\)</div>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1580894894513-541e068a3e2b?w=800&h=250&fit=crop" alt="Optimization and computation">
        <p class="image-caption">Modern sparse solvers make graph-based SLAM scalable to city-scale environments</p>
    </div>
</div>

<!-- ==================== PAGE 14: LOOP CLOSURE DETECTION ==================== -->
<div class="page">
    <h2>Loop Closure Detection</h2>

    <p>Loop closure is the process of recognising that the robot has returned to a previously visited location. It is <strong>critical</strong> for correcting accumulated drift and producing globally consistent maps.</p>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1497366216548-37526070297c?w=800&h=300&fit=crop" alt="Indoor environment with corridors">
        <p class="image-caption">Recognising a previously visited corridor enables the SLAM system to close the loop and correct drift</p>
    </div>

    <h3>Why Loop Closure Matters</h3>

    <div class="two-column">
        <div class="warning-box">
            <h4>Without Loop Closure</h4>
            <p>Drift accumulates along the trajectory. After a long loop, the start and end of the map do not align. Corridors appear duplicated and walls are inconsistent.</p>
        </div>
        <div class="info-box">
            <h4>With Loop Closure</h4>
            <p>A constraint is added between the current pose and the revisited pose. Graph optimisation distributes the accumulated error across the entire trajectory, producing a consistent map.</p>
        </div>
    </div>

    <!-- SVG: Before and After Loop Closure -->
    <div class="diagram">
        <svg viewBox="0 0 650 240" xmlns="http://www.w3.org/2000/svg" style="width:100%;height:auto;">
            <!-- Before loop closure (left) -->
            <text x="150" y="20" text-anchor="middle" font-size="13" fill="#ef5350" font-weight="bold">Before Loop Closure</text>
            <!-- Drifted trajectory -->
            <polyline points="40,180 80,180 120,140 160,100 200,80 220,100 240,140 240,180 200,200 160,200 120,210"
                      fill="none" stroke="#ef5350" stroke-width="2"/>
            <!-- Gap showing drift -->
            <circle cx="40" cy="180" r="6" fill="#667eea"/>
            <text x="40" y="170" text-anchor="middle" font-size="9" fill="#667eea">start</text>
            <circle cx="120" cy="210" r="6" fill="#ef5350"/>
            <text x="120" y="230" text-anchor="middle" font-size="9" fill="#ef5350">end (drifted)</text>
            <!-- Gap indicator -->
            <line x1="40" y1="180" x2="120" y2="210" stroke="#ef5350" stroke-width="1" stroke-dasharray="3,3"/>
            <text x="65" y="205" text-anchor="middle" font-size="9" fill="#ef5350">gap!</text>

            <!-- After loop closure (right) -->
            <text x="480" y="20" text-anchor="middle" font-size="13" fill="#4CAF50" font-weight="bold">After Loop Closure</text>
            <!-- Corrected trajectory (closed loop) -->
            <polyline points="370,180 410,180 450,140 490,100 530,80 550,100 570,140 570,180 530,200 490,200 450,200 410,195 370,180"
                      fill="none" stroke="#4CAF50" stroke-width="2"/>
            <!-- Closed -->
            <circle cx="370" cy="180" r="6" fill="#4CAF50" stroke="#333" stroke-width="1"/>
            <text x="370" y="170" text-anchor="middle" font-size="9" fill="#4CAF50">start = end</text>
        </svg>
    </div>

    <h3>Loop Closure Detection Methods</h3>

    <table>
        <tr><th>Method</th><th>Description</th><th>Pros / Cons</th></tr>
        <tr>
            <td><strong>Scan-to-Map Matching</strong></td>
            <td>Match current scan against the existing map at candidate locations</td>
            <td>Accurate; computationally expensive</td>
        </tr>
        <tr>
            <td><strong>Scan Descriptor Matching</strong></td>
            <td>Compute a compact descriptor (e.g., scan context) for each scan and match in descriptor space</td>
            <td>Fast retrieval; may miss subtle loops</td>
        </tr>
        <tr>
            <td><strong>Spatial Proximity</strong></td>
            <td>If the robot returns near a previous pose (within threshold), attempt ICP verification</td>
            <td>Simple; requires rough localisation</td>
        </tr>
        <tr>
            <td><strong>Branch and Bound</strong></td>
            <td>Used by Cartographer; multi-resolution correlative scan matching over candidate poses</td>
            <td>Globally optimal within search space; robust</td>
        </tr>
    </table>

    <div class="activity-box">
        <h3>Activity 3: Understanding Loop Closure Impact</h3>
        <p><strong>Objective:</strong> Visualise the effect of loop closure on pose graph optimisation.</p>
        <ol>
            <li>Create a simple pose graph in Python with 20 poses forming a square loop</li>
            <li>Add odometry edges with small Gaussian noise</li>
            <li>Observe the accumulated drift at the end of the loop (no loop closure)</li>
            <li>Add a loop closure edge connecting the last pose to the first</li>
            <li>Run a simple Gauss-Newton optimiser (3-5 iterations) and observe how the error distributes</li>
            <li>Plot the trajectory before and after optimisation</li>
        </ol>
    </div>

    <div class="warning-box">
        <strong>False Loop Closures:</strong> An incorrect loop closure detection (false positive) will introduce a wrong constraint and can cause catastrophic map distortion. Robust backends (e.g., using switchable constraints or max-mixtures) can mitigate this risk.
    </div>
</div>

<!-- ==================== PAGE 15: POPULAR SLAM ALGORITHMS ==================== -->
<div class="page">
    <h2>Popular 2D SLAM Algorithms</h2>

    <p>Several mature SLAM implementations are available as open-source ROS/ROS2 packages. Understanding their characteristics helps you choose the right tool for your application.</p>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1485827404703-89b55fcc595e?w=800&h=300&fit=crop" alt="Autonomous robot with sensors">
        <p class="image-caption">Different SLAM algorithms suit different robot platforms and environments</p>
    </div>

    <table>
        <tr>
            <th>Algorithm</th>
            <th>Type</th>
            <th>Sensor Requirements</th>
            <th>Key Features</th>
            <th>Use Cases</th>
        </tr>
        <tr>
            <td><strong>GMapping</strong></td>
            <td>Particle Filter (RBPF)</td>
            <td>2D LiDAR + Odometry</td>
            <td>Rao-Blackwellised; adaptive resampling; well-tested</td>
            <td>Indoor, small-medium environments</td>
        </tr>
        <tr>
            <td><strong>Hector SLAM</strong></td>
            <td>Scan Matching</td>
            <td>2D LiDAR only (no odometry needed)</td>
            <td>Gauss-Newton scan matching; high update rate; no odometry required</td>
            <td>UAVs, robots without wheel encoders</td>
        </tr>
        <tr>
            <td><strong>Cartographer</strong></td>
            <td>Graph-Based</td>
            <td>2D/3D LiDAR + IMU (optional)</td>
            <td>Correlative scan matching; branch-and-bound loop closure; submaps</td>
            <td>Large-scale, real-time, industrial</td>
        </tr>
        <tr>
            <td><strong>slam_toolbox</strong></td>
            <td>Graph-Based</td>
            <td>2D LiDAR + Odometry</td>
            <td>Karto-based; lifelong mapping; serialisation; Nav2 default</td>
            <td>ROS2 Nav2 stack, production robots</td>
        </tr>
        <tr>
            <td><strong>RTAB-Map</strong></td>
            <td>Graph-Based (multi-session)</td>
            <td>2D/3D LiDAR, RGB-D, stereo</td>
            <td>Appearance-based loop closure; memory management; multi-session</td>
            <td>Long-term autonomy, visual+LiDAR fusion</td>
        </tr>
    </table>

    <h3>Algorithm Selection Guide</h3>

    <div class="three-column">
        <div class="sensor-card">
            <h4>Small Indoor Robot</h4>
            <p><strong>Recommended:</strong> slam_toolbox or GMapping</p>
            <p>Simple setup, reliable for rooms and corridors</p>
        </div>
        <div class="sensor-card">
            <h4>Drone / No Odometry</h4>
            <p><strong>Recommended:</strong> Hector SLAM</p>
            <p>Works with LiDAR only at high scan rates</p>
        </div>
        <div class="sensor-card">
            <h4>Large Warehouse</h4>
            <p><strong>Recommended:</strong> Cartographer</p>
            <p>Handles large maps with robust loop closure</p>
        </div>
    </div>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1586528116311-ad8dd3c8310d?w=800&h=300&fit=crop" alt="Warehouse environment for robot mapping">
        <p class="image-caption">Large warehouse environments require SLAM algorithms with robust loop closure and efficient memory management</p>
    </div>

    <div class="tip">
        In ROS2 Humble and later, <code>slam_toolbox</code> is the default SLAM package integrated with the Nav2 navigation stack. It supports both synchronous and asynchronous modes, as well as lifelong mapping (serialise, deserialise, and continue mapping).
    </div>
</div>

<!-- ==================== PAGE 16: SLAM IN ROS2 ==================== -->
<div class="page">
    <h2>SLAM in ROS2</h2>

    <p>The ROS2 ecosystem provides ready-to-use SLAM packages. The primary tool is <code>slam_toolbox</code>, which integrates seamlessly with Nav2.</p>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1531746790095-e5995f3be4fe?w=800&h=300&fit=crop" alt="Software development and coding">
        <p class="image-caption">ROS2 packages provide production-ready SLAM implementations with configurable parameters</p>
    </div>

    <h3>Key Packages</h3>

    <table>
        <tr><th>Package</th><th>Purpose</th></tr>
        <tr><td><code>slam_toolbox</code></td><td>Online/offline 2D SLAM with graph-based optimisation</td></tr>
        <tr><td><code>nav2_map_server</code></td><td>Save and serve occupancy grid maps (PGM + YAML)</td></tr>
        <tr><td><code>tf2_ros</code></td><td>Transform tree management (map &rarr; odom &rarr; base_link)</td></tr>
        <tr><td><code>nav2_lifecycle_manager</code></td><td>Manages node lifecycle (configure, activate, deactivate)</td></tr>
    </table>

    <h3>slam_toolbox Launch File</h3>

    <pre>
# slam_launch.py
from launch import LaunchDescription
from launch_ros.actions import Node
from ament_index_python.packages import get_package_share_directory
import os

def generate_launch_description():
    slam_params = os.path.join(
        get_package_share_directory('my_slam_pkg'),
        'config', 'slam_toolbox_params.yaml'
    )

    return LaunchDescription([
        Node(
            package='slam_toolbox',
            executable='async_slam_toolbox_node',
            name='slam_toolbox',
            output='screen',
            parameters=[slam_params],
            remappings=[('/scan', '/lidar/scan')]
        ),
    ])
    </pre>

    <h3>Configuration Parameters (YAML)</h3>

    <pre>
slam_toolbox:
  ros__parameters:
    # General
    odom_frame: odom
    map_frame: map
    base_frame: base_link
    scan_topic: /scan
    mode: mapping  # or localization

    # Solver
    solver_plugin: solver_plugins::CeresSolver
    ceres_linear_solver: SPARSE_NORMAL_CHOLESKY
    ceres_preconditioner: SCHUR_JACOBI

    # Map
    resolution: 0.05              # 5 cm per cell
    max_laser_range: 12.0         # metres
    minimum_travel_distance: 0.5  # metres before new node
    minimum_travel_heading: 0.5   # radians before new node

    # Loop closure
    do_loop_closing: true
    loop_match_minimum_chain_size: 10
    loop_match_maximum_variance_coarse: 3.0

    # Correlation scan matching
    correlation_search_space_dimension: 0.5
    correlation_search_space_resolution: 0.01
    correlation_search_space_smear_deviation: 0.1
    </pre>

    <h3>Map Format: PGM + YAML</h3>

    <div class="two-column">
        <div>
            <h4>PGM Image</h4>
            <p>Greyscale image where:</p>
            <ul>
                <li>White (254) = Free space</li>
                <li>Black (0) = Occupied</li>
                <li>Grey (205) = Unknown</li>
            </ul>
        </div>
        <div>
            <h4>YAML Metadata</h4>
            <pre>
image: my_map.pgm
resolution: 0.050000
origin: [-10.0, -10.0, 0.0]
negate: 0
occupied_thresh: 0.65
free_thresh: 0.196
            </pre>
        </div>
    </div>

    <h3>Saving and Loading Maps</h3>

    <pre>
# Save the current map
ros2 run nav2_map_server map_saver_cli -f ~/maps/my_map --ros-args -p save_map_timeout:=5000

# Serve a saved map
ros2 run nav2_map_server map_server --ros-args -p yaml_filename:=~/maps/my_map.yaml
    </pre>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1517077304055-6e89abbf09b0?w=800&h=250&fit=crop" alt="Digital mapping technology">
        <p class="image-caption">The saved occupancy grid map can be used for autonomous navigation with Nav2</p>
    </div>
</div>

<!-- ==================== PAGE 17: SLAM EVALUATION METRICS ==================== -->
<div class="page">
    <h2>SLAM Evaluation Metrics</h2>

    <p>Quantitative evaluation of SLAM systems requires comparing the estimated trajectory and map against ground truth. Two standard metrics are widely used.</p>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1460925895917-afdab827c52f?w=800&h=300&fit=crop" alt="Data analysis and metrics">
        <p class="image-caption">Rigorous evaluation metrics enable objective comparison of SLAM algorithms</p>
    </div>

    <h3>Absolute Trajectory Error (ATE)</h3>

    <p>ATE measures the <strong>global consistency</strong> of the estimated trajectory. It computes the root mean square of pose differences after alignment:</p>

    <div class="math">
        \[ \text{ATE}_{\text{RMSE}} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} \| \text{trans}(S \, P_i^{-1} \, Q_i) \|^2} \]
    </div>

    <p>where \(P_i\) is the estimated pose, \(Q_i\) is the ground truth pose, and \(S\) is the rigid alignment (Umeyama alignment) between the two trajectories.</p>

    <div class="info-box">
        <strong>Interpretation:</strong> ATE captures overall map quality. A low ATE means the trajectory (and therefore the map) is globally accurate. Units are in metres.
    </div>

    <h3>Relative Pose Error (RPE)</h3>

    <p>RPE measures <strong>local accuracy</strong> (drift) by comparing relative transformations over fixed intervals \(\Delta\):</p>

    <div class="math">
        \[ E_i = (P_i^{-1} P_{i+\Delta})^{-1} (Q_i^{-1} Q_{i+\Delta}) \]
    </div>

    <div class="math">
        \[ \text{RPE}_{\text{RMSE}} = \sqrt{\frac{1}{M} \sum_{i=1}^{M} \| \text{trans}(E_i) \|^2} \]
    </div>
    <p><strong>where:</strong></p>
    <ul>
        <li>\( E_i \) — relative pose error at index \(i\)</li>
        <li>\( \Delta \) — fixed time or distance interval between compared poses</li>
        <li>\( M \) — number of relative pose pairs evaluated</li>
        <li>\( \text{trans}(\cdot) \) — extracts the translational component of a pose</li>
    </ul>

    <div class="info-box">
        <strong>Interpretation:</strong> RPE captures drift rate. Typically reported as m/m (translational) or deg/m (rotational). Useful for comparing odometry quality and short-term accuracy.
    </div>

    <h3>Map Quality Metrics</h3>

    <table>
        <tr><th>Metric</th><th>Formula / Description</th><th>What it Measures</th></tr>
        <tr>
            <td><strong>Map Score</strong></td>
            <td>Average scan-to-map matching score across all scans</td>
            <td>Internal consistency</td>
        </tr>
        <tr>
            <td><strong>Structural Similarity (SSIM)</strong></td>
            <td>SSIM between estimated map image and ground truth floor plan</td>
            <td>Visual map quality</td>
        </tr>
        <tr>
            <td><strong>Occupied Cell Accuracy</strong></td>
            <td>\(\frac{\text{TP}_{\text{occ}} + \text{TN}_{\text{free}}}{\text{Total cells}}\)</td>
            <td>Cell-level correctness</td>
        </tr>
        <tr>
            <td><strong>Entropy</strong></td>
            <td>\( H = -\sum_i [p_i \log p_i + (1-p_i) \log(1-p_i)] \)</td>
            <td>Remaining uncertainty in map</td>
        </tr>
    </table>

    <h3>Using evo for Trajectory Evaluation</h3>

    <pre>
# Install evo (trajectory evaluation tool)
pip install evo

# Compute ATE
evo_ape tum ground_truth.txt estimated.txt -p --plot_mode=xz -a

# Compute RPE
evo_rpe tum ground_truth.txt estimated.txt -p --delta 1 --delta_unit m
    </pre>

    <div class="tip">
        The <code>evo</code> Python package by Michael Grupp is the de facto standard for SLAM trajectory evaluation. It supports TUM, KITTI, EuRoC, and bag2 formats out of the box.
    </div>

    <div class="activity-box">
        <h3>Activity 4: Computing ATE and RPE</h3>
        <p><strong>Objective:</strong> Evaluate SLAM trajectory accuracy using ground truth data.</p>
        <ol>
            <li>Generate a ground truth circular trajectory (radius 5 m, 100 poses)</li>
            <li>Create an "estimated" trajectory by adding cumulative Gaussian noise (simulating drift)</li>
            <li>Compute ATE: align the trajectories using SVD, then compute RMSE of position errors</li>
            <li>Compute RPE with \(\Delta = 5\) poses: compare relative transformations</li>
            <li>Add a simulated loop closure correction (reduce error at the end) and recompute both metrics</li>
            <li>Discuss: which metric improved more with loop closure and why?</li>
        </ol>
    </div>
</div>

<!-- ==================== PAGE 18: COMMON SLAM FAILURE MODES ==================== -->
<div class="page">
    <h2>Common SLAM Failure Modes</h2>

    <p>Understanding how and why SLAM fails is as important as understanding how it works. Awareness of failure modes helps you design robust systems and diagnose problems in the field.</p>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1504328345606-18bbc8c9d7d1?w=800&h=300&fit=crop" alt="Long featureless corridor">
        <p class="image-caption">Long, featureless corridors are among the most challenging environments for LiDAR-based SLAM</p>
    </div>

    <h3>1. Featureless Environments</h3>

    <div class="warning-box">
        <strong>Problem:</strong> Long corridors, open fields, and symmetric rooms lack distinctive geometric features. Scan matching becomes ambiguous, and the robot can "slip" along the corridor axis.
        <br><br>
        <strong>Mitigation:</strong> Fuse with IMU or visual odometry. Use wheel encoders to constrain along-corridor motion. Place artificial landmarks if possible.
    </div>

    <h3>2. Dynamic Objects</h3>

    <div class="warning-box">
        <strong>Problem:</strong> Moving people, vehicles, or doors create inconsistent observations. A person walking through the scan appears as a transient obstacle, corrupting both scan matching and the map.
        <br><br>
        <strong>Mitigation:</strong> Use dynamic object filters (e.g., compare consecutive scans to identify moving points). Apply occupancy grid decay so that transient obstacles fade over time.
    </div>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1517457373958-b7bdd4587205?w=800&h=250&fit=crop" alt="Crowded environment with people">
        <p class="image-caption">Dynamic environments with pedestrians challenge SLAM systems that assume a static world</p>
    </div>

    <h3>3. Sensor Degradation</h3>

    <div class="warning-box">
        <strong>Problem:</strong> LiDAR performance degrades in dust, fog, rain, or when the sensor window is dirty. Reduced range and noisy returns lead to poor scan matching and incorrect map updates.
        <br><br>
        <strong>Mitigation:</strong> Monitor scan quality metrics (number of valid returns, range statistics). Fall back to odometry-only when LiDAR quality drops. Use sensor fusion with IMU.
    </div>

    <h3>4. Aggressive Motion</h3>

    <div class="warning-box">
        <strong>Problem:</strong> Rapid rotation or high-speed motion causes motion distortion in the LiDAR scan (each point is captured at a slightly different pose). This violates the rigid scan assumption.
        <br><br>
        <strong>Mitigation:</strong> Apply motion compensation using IMU data to de-skew the scan before processing. Limit robot speed during mapping.
    </div>

    <h3>5. False Loop Closures</h3>

    <div class="warning-box">
        <strong>Problem:</strong> Perceptual aliasing (different places that look similar) can cause incorrect loop closure detections, catastrophically distorting the map.
        <br><br>
        <strong>Mitigation:</strong> Use geometric verification after descriptor matching. Apply robust cost functions (Huber, Cauchy) in the graph optimizer. Use switchable constraints to down-weight suspect edges.
    </div>

    <h3>Failure Mode Summary</h3>

    <table>
        <tr><th>Failure Mode</th><th>Root Cause</th><th>Observable Symptom</th><th>Primary Mitigation</th></tr>
        <tr><td>Featureless</td><td>Ambiguous scan matching</td><td>Drift along corridor</td><td>IMU fusion, wheel encoders</td></tr>
        <tr><td>Dynamic objects</td><td>Static world assumption violated</td><td>Ghost obstacles in map</td><td>Dynamic object filter</td></tr>
        <tr><td>Sensor degradation</td><td>Noise, reduced range</td><td>Map blurring, divergence</td><td>Quality monitoring, fallback</td></tr>
        <tr><td>Aggressive motion</td><td>Scan distortion</td><td>Smeared features</td><td>Motion compensation (IMU)</td></tr>
        <tr><td>False loop closure</td><td>Perceptual aliasing</td><td>Map distortion</td><td>Robust back-end</td></tr>
    </table>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1563770660941-20978e870e26?w=800&h=250&fit=crop" alt="Industrial environment">
        <p class="image-caption">Industrial environments often combine multiple challenges: dust, dynamic machinery, and repetitive structures</p>
    </div>
</div>

<!-- ==================== PAGE 19: SUMMARY & LAB PREVIEW ==================== -->
<div class="page">
    <h2>Week 4 Summary</h2>

    <div class="info-box">
        <h4>Key Takeaways</h4>
        <ul>
            <li>SLAM simultaneously estimates the robot pose and environment map from sensor data</li>
            <li>Occupancy grids with log-odds provide efficient probabilistic map representation</li>
            <li>ICP and correlative scan matching align consecutive LiDAR scans to estimate motion</li>
            <li>Particle filter SLAM (GMapping) uses Rao-Blackwellisation to factorise the problem</li>
            <li>Graph-based SLAM (Cartographer, slam_toolbox) formulates SLAM as nonlinear least squares on a pose graph</li>
            <li>Loop closure is essential for globally consistent maps; false closures must be guarded against</li>
            <li>ROS2's slam_toolbox provides production-ready 2D SLAM integrated with Nav2</li>
        </ul>
    </div>

    <h2>Lab Preview: 7 Tasks</h2>

    <div class="checklist">
        <li><strong>Task 1:</strong> Implement a log-odds occupancy grid and integrate simulated LiDAR scans using Bresenham's algorithm</li>
        <li><strong>Task 2:</strong> Implement point-to-point ICP in Python and align two synthetic scans</li>
        <li><strong>Task 3:</strong> Simulate the odometry motion model and visualise pose uncertainty with 200 particle trajectories</li>
        <li><strong>Task 4:</strong> Build a minimal pose graph with 20 nodes and implement Gauss-Newton optimisation</li>
        <li><strong>Task 5:</strong> Launch slam_toolbox in Gazebo with a TurtleBot3 and teleoperate to build a map</li>
        <li><strong>Task 6:</strong> Save the map using nav2_map_server and inspect the PGM/YAML output</li>
        <li><strong>Task 7:</strong> Compare SLAM with and without loop closure; compute ATE using the evo tool</li>
    </div>

    <h2>Key Equations Reference Card</h2>

    <table>
        <tr><th>Concept</th><th>Equation</th></tr>
        <tr>
            <td>Log-odds update</td>
            <td>\( l_t(c_i) = l_{t-1}(c_i) + l_{\text{sensor}}(c_i) \)</td>
        </tr>
        <tr>
            <td>Sigmoid (log-odds to probability)</td>
            <td>\( p = \sigma(l) = \frac{1}{1+e^{-l}} \)</td>
        </tr>
        <tr>
            <td>ICP objective (point-to-point)</td>
            <td>\( E = \sum_i \|Rp_i + t - q_i\|^2 \)</td>
        </tr>
        <tr>
            <td>SVD rotation solution</td>
            <td>\( R^* = V U^\top \) from SVD of \( H = \sum p_i' q_i'^\top \)</td>
        </tr>
        <tr>
            <td>Graph SLAM cost</td>
            <td>\( F(\mathbf{x}) = \sum_{ij} e_{ij}^\top \Omega_{ij} \, e_{ij} \)</td>
        </tr>
        <tr>
            <td>Normal equations</td>
            <td>\( H \Delta\mathbf{x} = -\mathbf{b} \)</td>
        </tr>
        <tr>
            <td>ATE RMSE</td>
            <td>\( \sqrt{\frac{1}{N}\sum_i \|\text{trans}(S P_i^{-1} Q_i)\|^2} \)</td>
        </tr>
    </table>

    <h2>Interview / Exam Questions</h2>

    <ol>
        <li>Explain why SLAM is called a "chicken-and-egg" problem. How do modern algorithms address this?</li>
        <li>Derive the log-odds update rule from Bayes' theorem. Why is it preferred over direct probability updates?</li>
        <li>Describe the ICP algorithm step by step. What are its failure modes?</li>
        <li>Compare particle filter SLAM (GMapping) with graph-based SLAM (Cartographer). When would you use each?</li>
        <li>What is loop closure and why is it critical for large-scale mapping?</li>
        <li>Explain the difference between ATE and RPE. What does each metric tell you about SLAM quality?</li>
        <li>How does Google Cartographer achieve real-time performance using correlative scan matching?</li>
        <li>Name three SLAM failure modes and explain how to mitigate each.</li>
    </ol>

    <div class="image-container">
        <img src="https://images.unsplash.com/photo-1454165804606-c3d57bc86b40?w=800&h=250&fit=crop" alt="Study and preparation">
        <p class="image-caption">Review the key equations and concepts to prepare for the lab session and assessments</p>
    </div>
</div>

</body>
</html>
